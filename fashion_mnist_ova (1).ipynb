{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model for class 0: T-shirt/top vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.116054\n",
      "Cost after iteration 200: 0.110959\n",
      "Cost after iteration 300: 0.108221\n",
      "Cost after iteration 400: 0.106463\n",
      "Cost after iteration 500: 0.105232\n",
      "Cost after iteration 600: 0.104316\n",
      "Cost after iteration 700: 0.103603\n",
      "Cost after iteration 800: 0.103027\n",
      "Cost after iteration 900: 0.102548\n",
      "Cost after iteration 1000: 0.102141\n",
      "Cost after iteration 1100: 0.101788\n",
      "Cost after iteration 1200: 0.101476\n",
      "Cost after iteration 1300: 0.101197\n",
      "Cost after iteration 1400: 0.100946\n",
      "Cost after iteration 1500: 0.100717\n",
      "Cost after iteration 1600: 0.100506\n",
      "Cost after iteration 1700: 0.100312\n",
      "Cost after iteration 1800: 0.100132\n",
      "Cost after iteration 1900: 0.099964\n",
      "\n",
      "--- Training model for class 1: Trouser vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.042528\n",
      "Cost after iteration 200: 0.035553\n",
      "Cost after iteration 300: 0.032043\n",
      "Cost after iteration 400: 0.029849\n",
      "Cost after iteration 500: 0.028323\n",
      "Cost after iteration 600: 0.027185\n",
      "Cost after iteration 700: 0.026294\n",
      "Cost after iteration 800: 0.025570\n",
      "Cost after iteration 900: 0.024967\n",
      "Cost after iteration 1000: 0.024452\n",
      "Cost after iteration 1100: 0.024006\n",
      "Cost after iteration 1200: 0.023614\n",
      "Cost after iteration 1300: 0.023265\n",
      "Cost after iteration 1400: 0.022951\n",
      "Cost after iteration 1500: 0.022668\n",
      "Cost after iteration 1600: 0.022409\n",
      "Cost after iteration 1700: 0.022172\n",
      "Cost after iteration 1800: 0.021953\n",
      "Cost after iteration 1900: 0.021750\n",
      "\n",
      "--- Training model for class 2: Pullover vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.217604\n",
      "Cost after iteration 200: 0.342158\n",
      "Cost after iteration 300: 0.309055\n",
      "Cost after iteration 400: 0.292742\n",
      "Cost after iteration 500: 0.282521\n",
      "Cost after iteration 600: 0.275186\n",
      "Cost after iteration 700: 0.269519\n",
      "Cost after iteration 800: 0.264945\n",
      "Cost after iteration 900: 0.261149\n",
      "Cost after iteration 1000: 0.257935\n",
      "Cost after iteration 1100: 0.255171\n",
      "Cost after iteration 1200: 0.252767\n",
      "Cost after iteration 1300: 0.250653\n",
      "Cost after iteration 1400: 0.248778\n",
      "Cost after iteration 1500: 0.247102\n",
      "Cost after iteration 1600: 0.245593\n",
      "Cost after iteration 1700: 0.244225\n",
      "Cost after iteration 1800: 0.242979\n",
      "Cost after iteration 1900: 0.241837\n",
      "\n",
      "--- Training model for class 3: Dress vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.101145\n",
      "Cost after iteration 200: 0.094670\n",
      "Cost after iteration 300: 0.091633\n",
      "Cost after iteration 400: 0.089689\n",
      "Cost after iteration 500: 0.088272\n",
      "Cost after iteration 600: 0.087169\n",
      "Cost after iteration 700: 0.086274\n",
      "Cost after iteration 800: 0.085526\n",
      "Cost after iteration 900: 0.084887\n",
      "Cost after iteration 1000: 0.084333\n",
      "Cost after iteration 1100: 0.083844\n",
      "Cost after iteration 1200: 0.083408\n",
      "Cost after iteration 1300: 0.083016\n",
      "Cost after iteration 1400: 0.082660\n",
      "Cost after iteration 1500: 0.082335\n",
      "Cost after iteration 1600: 0.082037\n",
      "Cost after iteration 1700: 0.081761\n",
      "Cost after iteration 1800: 0.081505\n",
      "Cost after iteration 1900: 0.081267\n",
      "\n",
      "--- Training model for class 4: Coat vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.361901\n",
      "Cost after iteration 200: 0.300055\n",
      "Cost after iteration 300: 0.269468\n",
      "Cost after iteration 400: 0.250199\n",
      "Cost after iteration 500: 0.238420\n",
      "Cost after iteration 600: 0.230835\n",
      "Cost after iteration 700: 0.225478\n",
      "Cost after iteration 800: 0.221361\n",
      "Cost after iteration 900: 0.218002\n",
      "Cost after iteration 1000: 0.215150\n",
      "Cost after iteration 1100: 0.212669\n",
      "Cost after iteration 1200: 0.210471\n",
      "Cost after iteration 1300: 0.208501\n",
      "Cost after iteration 1400: 0.206718\n",
      "Cost after iteration 1500: 0.205093\n",
      "Cost after iteration 1600: 0.203603\n",
      "Cost after iteration 1700: 0.202229\n",
      "Cost after iteration 1800: 0.200958\n",
      "Cost after iteration 1900: 0.199777\n",
      "\n",
      "--- Training model for class 5: Sandal vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.087949\n",
      "Cost after iteration 200: 0.073176\n",
      "Cost after iteration 300: 0.066132\n",
      "Cost after iteration 400: 0.061853\n",
      "Cost after iteration 500: 0.058916\n",
      "Cost after iteration 600: 0.056746\n",
      "Cost after iteration 700: 0.055060\n",
      "Cost after iteration 800: 0.053702\n",
      "Cost after iteration 900: 0.052579\n",
      "Cost after iteration 1000: 0.051631\n",
      "Cost after iteration 1100: 0.050816\n",
      "Cost after iteration 1200: 0.050106\n",
      "Cost after iteration 1300: 0.049480\n",
      "Cost after iteration 1400: 0.048923\n",
      "Cost after iteration 1500: 0.048423\n",
      "Cost after iteration 1600: 0.047972\n",
      "Cost after iteration 1700: 0.047560\n",
      "Cost after iteration 1800: 0.047184\n",
      "Cost after iteration 1900: 0.046838\n",
      "\n",
      "--- Training model for class 6: Shirt vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.746584\n",
      "Cost after iteration 200: 0.904343\n",
      "Cost after iteration 300: 0.364095\n",
      "Cost after iteration 400: 0.225589\n",
      "Cost after iteration 500: 0.358552\n",
      "Cost after iteration 600: 0.454881\n",
      "Cost after iteration 700: 0.235818\n",
      "Cost after iteration 800: 0.342499\n",
      "Cost after iteration 900: 0.631842\n",
      "Cost after iteration 1000: 0.681184\n",
      "Cost after iteration 1100: 0.283274\n",
      "Cost after iteration 1200: 0.320668\n",
      "Cost after iteration 1300: 0.306227\n",
      "Cost after iteration 1400: 0.194558\n",
      "Cost after iteration 1500: 0.288739\n",
      "Cost after iteration 1600: 0.187329\n",
      "Cost after iteration 1700: 0.186571\n",
      "Cost after iteration 1800: 0.276613\n",
      "Cost after iteration 1900: 0.185346\n",
      "\n",
      "--- Training model for class 7: Sneaker vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.080156\n",
      "Cost after iteration 200: 0.069316\n",
      "Cost after iteration 300: 0.063897\n",
      "Cost after iteration 400: 0.060497\n",
      "Cost after iteration 500: 0.058117\n",
      "Cost after iteration 600: 0.056337\n",
      "Cost after iteration 700: 0.054944\n",
      "Cost after iteration 800: 0.053817\n",
      "Cost after iteration 900: 0.052881\n",
      "Cost after iteration 1000: 0.052090\n",
      "Cost after iteration 1100: 0.051409\n",
      "Cost after iteration 1200: 0.050815\n",
      "Cost after iteration 1300: 0.050291\n",
      "Cost after iteration 1400: 0.049825\n",
      "Cost after iteration 1500: 0.049407\n",
      "Cost after iteration 1600: 0.049029\n",
      "Cost after iteration 1700: 0.048685\n",
      "Cost after iteration 1800: 0.048370\n",
      "Cost after iteration 1900: 0.048081\n",
      "\n",
      "--- Training model for class 8: Bag vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.074413\n",
      "Cost after iteration 200: 0.065302\n",
      "Cost after iteration 300: 0.060524\n",
      "Cost after iteration 400: 0.057390\n",
      "Cost after iteration 500: 0.055144\n",
      "Cost after iteration 600: 0.053448\n",
      "Cost after iteration 700: 0.052118\n",
      "Cost after iteration 800: 0.051044\n",
      "Cost after iteration 900: 0.050156\n",
      "Cost after iteration 1000: 0.049409\n",
      "Cost after iteration 1100: 0.048769\n",
      "Cost after iteration 1200: 0.048213\n",
      "Cost after iteration 1300: 0.047725\n",
      "Cost after iteration 1400: 0.047292\n",
      "Cost after iteration 1500: 0.046904\n",
      "Cost after iteration 1600: 0.046554\n",
      "Cost after iteration 1700: 0.046237\n",
      "Cost after iteration 1800: 0.045946\n",
      "Cost after iteration 1900: 0.045679\n",
      "\n",
      "--- Training model for class 9: Ankle boot vs. rest ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.069282\n",
      "Cost after iteration 200: 0.058661\n",
      "Cost after iteration 300: 0.053114\n",
      "Cost after iteration 400: 0.049477\n",
      "Cost after iteration 500: 0.046850\n",
      "Cost after iteration 600: 0.044844\n",
      "Cost after iteration 700: 0.043254\n",
      "Cost after iteration 800: 0.041960\n",
      "Cost after iteration 900: 0.040883\n",
      "Cost after iteration 1000: 0.039973\n",
      "Cost after iteration 1100: 0.039191\n",
      "Cost after iteration 1200: 0.038512\n",
      "Cost after iteration 1300: 0.037917\n",
      "Cost after iteration 1400: 0.037389\n",
      "Cost after iteration 1500: 0.036919\n",
      "Cost after iteration 1600: 0.036495\n",
      "Cost after iteration 1700: 0.036113\n",
      "Cost after iteration 1800: 0.035765\n",
      "Cost after iteration 1900: 0.035447\n",
      "train accuracy: 45.398333333333326 %\n",
      "test accuracy: 42.78999999999999 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Loading the data (fashion MNIST)\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "NUM_CLASSES  = len(CLASS_NAMES)\n",
    "\n",
    "m_train = 60000\n",
    "m_test  = 10000\n",
    "num_px  = 28\n",
    "\n",
    "train_set_x_flatten = train_images.reshape(train_images.shape[0], -1).T\n",
    "test_set_x_flatten  = test_images.reshape(test_images.shape[0], -1).T\n",
    "\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x  = test_set_x_flatten  / 255.\n",
    "\n",
    "train_set_y = train_labels.reshape(1, -1)\n",
    "test_set_y  = test_labels.reshape(1, -1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return s\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "\n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 or 1) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "\n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)                                                  # compute activation\n",
    "    cost = -(1/m) * np.sum(Y * np.log(A + 1e-8) + (1 - Y) * np.log(1 - A + 1e-8))   # compute cost\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1/m) * np.dot(X, (A - Y).T)\n",
    "    db = (1/m) * np.sum(A - Y, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads, cost\n",
    "\n",
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 or 1), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "\n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    # Compute vector \"A\" predicting the probabilities of the class being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0, i] <= 0.5:\n",
    "            Y_prediction[0, i] = 0\n",
    "        else:\n",
    "            Y_prediction[0, i] = 1\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "# GRADED FUNCTION: train_all_models\n",
    "\n",
    "def train_all_models(X_train, Y_train, num_classes, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Trains one binary logistic regression model per class using the one-vs-all approach.\n",
    "    For each class k, relabels Y so that class k is 1 and all others are 0,\n",
    "    then trains and saves the parameters using the optimize() function.\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px, m_train)\n",
    "    Y_train -- training labels represented by a numpy array of shape (1, m_train), values 0-9\n",
    "    num_classes -- number of classes to train (10 for fashion MNIST)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    all_params -- list of dictionaries, one per class, each containing the weights w and bias b\n",
    "    all_costs -- list of cost lists, one per class, used to plot the learning curves\n",
    "    \"\"\"\n",
    "\n",
    "    all_params = []\n",
    "    all_costs  = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for k in range(num_classes):\n",
    "        print (\"\\n--- Training model for class %i: %s vs. rest ---\" %(k, CLASS_NAMES[k]))\n",
    "\n",
    "        # Create binary label vector: 1 if class k, 0 otherwise (≈ 1 line of code)\n",
    "        Y_binary = (Y_train == k).astype(int)\n",
    "\n",
    "        # initialize parameters with zeros (≈ 1 line of code)\n",
    "        w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "        # Gradient descent (≈ 1 line of code)\n",
    "        parameters, grads, costs = optimize(w, b, X_train, Y_binary, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "        # Save the trained parameters and costs for this class (≈ 2 lines of code)\n",
    "        all_params.append(parameters)\n",
    "        all_costs.append(costs)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return all_params, all_costs\n",
    "\n",
    "# GRADED FUNCTION: predict_class\n",
    "\n",
    "def predict_class(all_params, X):\n",
    "    '''\n",
    "    Predict the class label for one or more images by running all one-vs-all models\n",
    "    and selecting the class with the highest predicted probability.\n",
    "\n",
    "    Arguments:\n",
    "    all_params -- list of dictionaries containing the weights w and bias b for each class,\n",
    "                  as returned by train_all_models\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    predicted_labels -- a numpy array of shape (m,) containing the predicted class (0-9) for each example\n",
    "    probabilities -- a numpy array of shape (num_classes, m) containing the sigmoid output of each model\n",
    "    '''\n",
    "\n",
    "    num_classes = len(all_params)\n",
    "    m = X.shape[1]\n",
    "    probabilities = np.zeros((num_classes, m))\n",
    "\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    # Compute the probability for each class by running each one-vs-all model\n",
    "    for k in range(num_classes):\n",
    "        w = all_params[k][\"w\"]\n",
    "        b = all_params[k][\"b\"]\n",
    "        probabilities[k, :] = sigmoid(np.dot(w.T, X) + b).flatten()\n",
    "\n",
    "    # Select the class with the highest probability\n",
    "    predicted_labels = np.argmax(probabilities, axis=0)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return predicted_labels, probabilities\n",
    "\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the one-vs-all logistic regression model by calling the functions you've implemented previously\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Train all one-vs-all models (≈ 1 line of code)\n",
    "    all_params, all_costs = train_all_models(X_train, Y_train, NUM_CLASSES, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_train, _ = predict_class(all_params, X_train)\n",
    "    Y_prediction_test,  _ = predict_class(all_params, X_test)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train.flatten())) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test.flatten())) * 100))\n",
    "\n",
    "\n",
    "    d = {\"all_costs\": all_costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\" : Y_prediction_train,\n",
    "         \"all_params\" : all_params,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d\n",
    "\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.5, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 10 Fashion MNIST classes, the original labels are converted into binary labels: samples of the target class are labeled 1, and all others are labeled 0. A separate logistic regression model is trained for each class using gradient descent, resulting in 10 independent models, each with its own weights and bias.\n",
    "\n",
    "During inference, an input image is evaluated by all 10 models. Each model outputs a probability that the image belongs to its class, and the final prediction is the class with the highest probability.\n",
    "\n",
    "Model performance is measured by computing accuracy on both the training and test datasets. Comparing these accuracies helps assess how well the model generalizes, with a large gap indicating potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2000 iterations of each item type, the results were\n",
    "\n",
    "Train accuracy: 45.398% \n",
    "Test accuracy: 42.78%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
